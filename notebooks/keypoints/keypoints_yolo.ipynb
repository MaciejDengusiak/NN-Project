{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"../../data/coco/\"  # change this\n",
    "IMG_DIR_TRAIN = os.path.join(COCO_PATH, \"images/train2017\")\n",
    "IMG_DIR_VAL = os.path.join(COCO_PATH, \"images/val2017\")\n",
    "ANN_FILE_TRAIN = os.path.join(COCO_PATH, \"annotations/person_keypoints_train2017.json\")\n",
    "ANN_FILE_VAL = os.path.join(COCO_PATH, \"annotations/person_keypoints_val2017.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\patry\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=10.49s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class YOLOKeypointDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transform=None, grid_size=32, num_keypoints=17):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.coco = self.coco\n",
    "        self.transform = transform\n",
    "        self.grid_size = grid_size\n",
    "        self.num_keypoints = num_keypoints\n",
    "        self.image_size = 256  # resize all images\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = super().__getitem__(index)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        target_tensor = torch.zeros((self.grid_size, self.grid_size, self.num_keypoints * 3))  # x, y, vis for each kp\n",
    "        for ann in target:\n",
    "            if ann[\"num_keypoints\"] == 0:\n",
    "                continue\n",
    "            kps = torch.tensor(ann[\"keypoints\"]).view(-1, 3)\n",
    "            for i, (x, y, v) in enumerate(kps):\n",
    "                if v > 0:\n",
    "                    grid_x = int(x * self.grid_size / self.image_size)\n",
    "                    grid_y = int(y * self.grid_size / self.image_size)\n",
    "                    if 0 <= grid_x < self.grid_size and 0 <= grid_y < self.grid_size:\n",
    "                        target_tensor[grid_y, grid_x, i*3 + 0] = x / self.image_size\n",
    "                        target_tensor[grid_y, grid_x, i*3 + 1] = y / self.image_size\n",
    "                        target_tensor[grid_y, grid_x, i*3 + 2] = 1.0  # visible\n",
    "        return img, target_tensor\n",
    "\n",
    "transform = T.Compose([\n",
    "    T.Resize((256, 256)),\n",
    "    T.ToTensor(),\n",
    "    T.ConvertImageDtype(torch.float32)\n",
    "])\n",
    "\n",
    "train_dataset = YOLOKeypointDataset(IMG_DIR_TRAIN, ANN_FILE_TRAIN, transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOKeypointNet(nn.Module):\n",
    "    def __init__(self, num_keypoints=17, grid_size=32):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet18(weights=\"DEFAULT\")\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(512, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, num_keypoints * 3, kernel_size=1)  # (x, y, vis) per keypoint\n",
    "        )\n",
    "        self.grid_size = grid_size\n",
    "        self.num_keypoints = num_keypoints\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = x.view(x.size(0), 512, 1, 1)\n",
    "        x = torch.nn.functional.interpolate(x, size=(self.grid_size, self.grid_size), mode=\"bilinear\")\n",
    "        x = self.head(x)\n",
    "        return x.permute(0, 2, 3, 1)  # [B, G, G, K*3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keypoint_loss(pred, target):\n",
    "    mask = target[..., 2::3] > 0  # visibility mask\n",
    "    pos_loss = ((pred[..., 0::3] - target[..., 0::3])**2 +\n",
    "                (pred[..., 1::3] - target[..., 1::3])**2)\n",
    "    conf_loss = ((pred[..., 2::3] - target[..., 2::3])**2)\n",
    "    total_loss = (pos_loss * mask).sum() + conf_loss.sum()\n",
    "    return total_loss / target.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mfejowo5522\u001b[0m (\u001b[33mfejowo5522-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\patry\\Documents\\Programming\\NN-Project\\notebooks\\keypoints\\wandb\\run-20250622_140521-d7aqt0g4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fejowo5522-/yolo-keypoint/runs/d7aqt0g4' target=\"_blank\">resnet18-grid32</a></strong> to <a href='https://wandb.ai/fejowo5522-/yolo-keypoint' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fejowo5522-/yolo-keypoint' target=\"_blank\">https://wandb.ai/fejowo5522-/yolo-keypoint</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fejowo5522-/yolo-keypoint/runs/d7aqt0g4' target=\"_blank\">https://wandb.ai/fejowo5522-/yolo-keypoint/runs/d7aqt0g4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "792abb98e2d54ef3a87826a6cee8a42f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/7393 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = YOLOKeypointNet().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "wandb.init(project=\"yolo-keypoint\", name=\"resnet18-grid32\", config={\"epochs\": 10, \"grid_size\": 32})\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    for imgs, targets in loop:\n",
    "        imgs = imgs.to(device)\n",
    "        targets = targets.to(device)\n",
    "\n",
    "        preds = model(imgs)\n",
    "        loss = keypoint_loss(preds, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "        wandb.log({\"loss\": loss.item()})\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, avg loss: {total_loss / len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(img, pred, threshold=0.5):\n",
    "    img = img.permute(1, 2, 0).cpu().numpy()\n",
    "    pred = pred.cpu().detach().numpy()\n",
    "\n",
    "    plt.imshow(img)\n",
    "    G = pred.shape[0]\n",
    "    for y in range(G):\n",
    "        for x in range(G):\n",
    "            for i in range(17):\n",
    "                conf = pred[y, x, i * 3 + 2]\n",
    "                if conf > threshold:\n",
    "                    px = pred[y, x, i * 3 + 0] * 256\n",
    "                    py = pred[y, x, i * 3 + 1] * 256\n",
    "                    plt.scatter([px], [py], c='r', s=10)\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
