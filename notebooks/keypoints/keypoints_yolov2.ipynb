{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db9c7856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from pycocotools.coco import COCO\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set paths (update these according to your setup)\n",
    "COCO_PATH = \"../../data/coco/\"\n",
    "IMG_DIR_TRAIN = os.path.join(COCO_PATH, \"images/train2017\")\n",
    "IMG_DIR_VAL = os.path.join(COCO_PATH, \"images/val2017\")\n",
    "ANN_FILE_TRAIN = os.path.join(COCO_PATH, \"annotations/person_keypoints_train2017.json\")\n",
    "ANN_FILE_VAL = os.path.join(COCO_PATH, \"annotations/person_keypoints_val2017.json\")\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "050aa448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=11.15s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.43s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class COCOKeypointsDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_file, img_size=416, grid_size=13, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.coco = COCO(ann_file)\n",
    "        self.img_ids = list(self.coco.imgs.keys())\n",
    "        self.img_size = img_size\n",
    "        self.grid_size = grid_size\n",
    "        self.transform = transform\n",
    "        self.cell_size = img_size / grid_size\n",
    "        \n",
    "        # Filter images with at least one person\n",
    "        valid_ids = []\n",
    "        for img_id in self.img_ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[1])  # Category 1: person\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            if len(anns) > 0:\n",
    "                valid_ids.append(img_id)\n",
    "        self.img_ids = valid_ids\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_info = self.coco.loadImgs(img_id)[0]\n",
    "        img_path = os.path.join(self.img_dir, img_info['file_name'])\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        # Resize and transform image\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Initialize target grid tensor\n",
    "        # Format: [obj, x, y, w, h, kp_x1, kp_y1, kp_v1, ...] for 17 keypoints\n",
    "        target = torch.zeros((self.grid_size, self.grid_size, 1 + 4 + 17 * 3))\n",
    "        \n",
    "        # Get annotations\n",
    "        ann_ids = self.coco.getAnnIds(imgIds=img_id, catIds=[1])\n",
    "        anns = self.coco.loadAnns(ann_ids)\n",
    "        \n",
    "        for ann in anns:\n",
    "            # Skip crowd annotations\n",
    "            if ann.get('iscrowd', 0):\n",
    "                continue\n",
    "                \n",
    "            # Get bounding box and keypoints\n",
    "            x, y, w, h = ann['bbox']\n",
    "            keypoints = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "            \n",
    "            # Scale to image_size\n",
    "            scale_x = self.img_size / orig_w\n",
    "            scale_y = self.img_size / orig_h\n",
    "            \n",
    "            # Scale bounding box\n",
    "            x = x * scale_x\n",
    "            y = y * scale_y\n",
    "            w = w * scale_x\n",
    "            h = h * scale_y\n",
    "            \n",
    "            # Calculate grid cell\n",
    "            cx = x + w / 2\n",
    "            cy = y + h / 2\n",
    "            grid_x = int(cx / self.cell_size)\n",
    "            grid_y = int(cy / self.cell_size)\n",
    "            \n",
    "            # Skip if out of bounds\n",
    "            if grid_x >= self.grid_size or grid_y >= self.grid_size:\n",
    "                continue\n",
    "                \n",
    "            # Objectness score\n",
    "            target[grid_y, grid_x, 0] = 1\n",
    "            \n",
    "            # Bounding box (relative to cell)\n",
    "            target[grid_y, grid_x, 1] = (cx - grid_x * self.cell_size) / self.cell_size\n",
    "            target[grid_y, grid_x, 2] = (cy - grid_y * self.cell_size) / self.cell_size\n",
    "            target[grid_y, grid_x, 3] = w / self.img_size\n",
    "            target[grid_y, grid_x, 4] = h / self.img_size\n",
    "            \n",
    "            # Keypoints\n",
    "            for k, (x_k, y_k, v_k) in enumerate(keypoints):\n",
    "                offset = 5 + k * 3\n",
    "                x_k = x_k * scale_x\n",
    "                y_k = y_k * scale_y\n",
    "                \n",
    "                # Keypoint coordinates relative to cell\n",
    "                target[grid_y, grid_x, offset] = (x_k - grid_x * self.cell_size) / self.cell_size\n",
    "                target[grid_y, grid_x, offset + 1] = (y_k - grid_y * self.cell_size) / self.cell_size\n",
    "                target[grid_y, grid_x, offset + 2] = 1 if v_k == 2 else 0  # 2=visible\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((416, 416)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = COCOKeypointsDataset(\n",
    "    IMG_DIR_TRAIN, ANN_FILE_TRAIN, transform=transform\n",
    ")\n",
    "val_dataset = COCOKeypointsDataset(\n",
    "    IMG_DIR_VAL, ANN_FILE_VAL, transform=transform\n",
    ")\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf11a98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointYOLO(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KeypointYOLO, self).__init__()\n",
    "        self.grid_size = 13\n",
    "        self.num_keypoints = 17\n",
    "        \n",
    "        # Feature extractor (simplified Darknet)\n",
    "        self.features = nn.Sequential(\n",
    "            # Input: 3x416x416\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),  # 208x208\n",
    "            \n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),  # 104x104\n",
    "            \n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),  # 52x52\n",
    "            \n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),  # 26x26\n",
    "            \n",
    "            nn.Conv2d(256, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.MaxPool2d(2, 2),  # 13x13\n",
    "        )\n",
    "        \n",
    "        # Detection head\n",
    "        self.detector = nn.Sequential(\n",
    "            nn.Conv2d(512, 1024, 3, padding=1),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Conv2d(1024, (1 + 4 + 3 * self.num_keypoints), 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.detector(x)\n",
    "        # Reshape: (batch, channels, grid, grid) -> (batch, grid, grid, channels)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20278497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeypointLoss(nn.Module):\n",
    "    def __init__(self, lambda_coord=5, lambda_noobj=0.5, lambda_kp=5):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss(reduction='sum')\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='sum')\n",
    "        self.lambda_coord = lambda_coord\n",
    "        self.lambda_noobj = lambda_noobj\n",
    "        self.lambda_kp = lambda_kp\n",
    "\n",
    "    def forward(self, preds, targets):\n",
    "        # Shapes: (batch, grid, grid, 1+4+51)\n",
    "        obj_mask = targets[..., 0] == 1  # Cells with objects\n",
    "        noobj_mask = targets[..., 0] == 0  # Cells without objects\n",
    "        \n",
    "        # Objectness loss\n",
    "        obj_loss = self.bce(preds[..., 0][obj_mask], targets[..., 0][obj_mask])\n",
    "        noobj_loss = self.bce(preds[..., 0][noobj_mask], targets[..., 0][noobj_mask])\n",
    "        \n",
    "        # Bounding box losses (only for cells with objects)\n",
    "        box_preds = preds[obj_mask][..., 1:5]\n",
    "        box_targets = targets[obj_mask][..., 1:5]\n",
    "        \n",
    "        # Center coordinates\n",
    "        center_loss = self.mse(box_preds[..., :2], box_targets[..., :2])\n",
    "        \n",
    "        # Width/height\n",
    "        wh_loss = self.mse(torch.sqrt(box_preds[..., 2:4] + 1e-6), \n",
    "                          torch.sqrt(box_targets[..., 2:4] + 1e-6))\n",
    "        \n",
    "        # Keypoint losses\n",
    "        kp_loss = 0\n",
    "        for k in range(17):\n",
    "            # Get predictions and targets for this keypoint\n",
    "            kp_preds = preds[obj_mask][..., 5 + k*3: 5 + k*3 + 2]\n",
    "            kp_targets = targets[obj_mask][..., 5 + k*3: 5 + k*3 + 2]\n",
    "            vis_targets = targets[obj_mask][..., 5 + k*3 + 2]\n",
    "            \n",
    "            # Only calculate loss for visible keypoints\n",
    "            vis_mask = vis_targets == 1\n",
    "            if vis_mask.sum() > 0:\n",
    "                kp_loss += self.mse(kp_preds[vis_mask], kp_targets[vis_mask])\n",
    "            \n",
    "            # Visibility classification\n",
    "            vis_loss = self.bce(preds[obj_mask][..., 5 + k*3 + 2], vis_targets)\n",
    "            kp_loss += vis_loss\n",
    "        \n",
    "        total_loss = (\n",
    "            obj_loss + \n",
    "            self.lambda_noobj * noobj_loss +\n",
    "            self.lambda_coord * (center_loss + wh_loss) +\n",
    "            self.lambda_kp * kp_loss\n",
    "        )\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9521122f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/8015 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-4):\n",
    "    model = model.to(device)\n",
    "    criterion = KeypointLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for images, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            images = images.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            preds = model(images)\n",
    "            loss = criterion(preds, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.to(device)\n",
    "                preds = model(images)\n",
    "                loss = criterion(preds, targets)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | \"\n",
    "              f\"Train Loss: {train_loss/len(train_loader):.4f} | \"\n",
    "              f\"Val Loss: {val_loss/len(val_loader):.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize and train model\n",
    "model = KeypointYOLO()\n",
    "trained_model = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs=10,\n",
    "    lr=1e-4\n",
    ")\n",
    "\n",
    "# Save model\n",
    "torch.save(trained_model.state_dict(), \"keypoint_yolo.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946e5c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image, preds, threshold=0.5):\n",
    "    # Convert image to numpy\n",
    "    img = image.permute(1, 2, 0).cpu().numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Process predictions\n",
    "    grid_size = preds.shape[0]\n",
    "    cell_size = 416 / grid_size\n",
    "    \n",
    "    for gy in range(grid_size):\n",
    "        for gx in range(grid_size):\n",
    "            if preds[gy, gx, 0] > threshold:  # Object detected\n",
    "                # Get bounding box\n",
    "                bx = (gx + preds[gy, gx, 1]) * cell_size\n",
    "                by = (gy + preds[gy, gx, 2]) * cell_size\n",
    "                bw = preds[gy, gx, 3] * 416\n",
    "                bh = preds[gy, gx, 4] * 416\n",
    "                \n",
    "                # Draw rectangle\n",
    "                rect = plt.Rectangle(\n",
    "                    (bx - bw/2, by - bh/2), bw, bh,\n",
    "                    fill=False, edgecolor='red', linewidth=1\n",
    "                )\n",
    "                ax.add_patch(rect)\n",
    "                \n",
    "                # Draw keypoints\n",
    "                for k in range(17):\n",
    "                    if preds[gy, gx, 5 + k*3 + 2] > 0.5:  # Visible keypoint\n",
    "                        kx = (gx + preds[gy, gx, 5 + k*3]) * cell_size\n",
    "                        ky = (gy + preds[gy, gx, 5 + k*3 + 1]) * cell_size\n",
    "                        ax.scatter(kx, ky, s=20, c='blue')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Load a sample image\n",
    "model.eval()\n",
    "sample_img, _ = next(iter(val_loader))\n",
    "with torch.no_grad():\n",
    "    preds = model(sample_img[0:1].to(device))[0].cpu().numpy()\n",
    "\n",
    "# Visualize\n",
    "visualize_predictions(sample_img[0], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9dcfc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
