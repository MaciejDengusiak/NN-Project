{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a94b0f69",
   "metadata": {},
   "source": [
    "# TODO\n",
    "- GT heatmaps are not correctly calculated "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374bb287",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f428154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torchvision.transforms import functional as F\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms.functional as VF\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import wandb\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8cabf73e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "386ffcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "\n",
    "# reduce cpu contention\n",
    "torch.set_num_threads(1)\n",
    "NUM_WORKERS = 6  # adjust based on CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe37be3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"../../data/coco/\"  # change this\n",
    "IMG_DIR_TRAIN = os.path.join(COCO_PATH, \"images/train2017\")\n",
    "IMG_DIR_VAL = os.path.join(COCO_PATH, \"images/val2017\")\n",
    "ANN_FILE_TRAIN = os.path.join(COCO_PATH, \"annotations/person_keypoints_train2017.json\")\n",
    "ANN_FILE_VAL = os.path.join(COCO_PATH, \"annotations/person_keypoints_val2017.json\")\n",
    "\n",
    "REMOVE_IMAGES_WITHOUT_KEYPOINTS = True\n",
    "VAL_SPLIT = 0.5\n",
    "TEST_VAL_TRAIN_PERCENT = (0.1, 0.1, 0.1)\n",
    "BATCH_SIZE = 128\n",
    "DATA_AUGMENTATION = False\n",
    "\n",
    "NUM_KEYPOINTS = 17\n",
    "\n",
    "KEYPOINT_NAMES = [\n",
    "    'nose',\n",
    "    'left_eye',\n",
    "    'right_eye',\n",
    "    'left_ear',\n",
    "    'right_ear',\n",
    "    'left_shoulder',\n",
    "    'right_shoulder',\n",
    "    'left_elbow',\n",
    "    'right_elbow',\n",
    "    'left_wrist',\n",
    "    'right_wrist',\n",
    "    'left_hip',\n",
    "    'right_hip',\n",
    "    'left_knee',\n",
    "    'right_knee',\n",
    "    'left_ankle',\n",
    "    'right_ankle'\n",
    "]\n",
    "\n",
    "\n",
    "HEATMAP_OUTPUT_STRIDE = 4\n",
    "HEATMAP_SIZE = (256 // HEATMAP_OUTPUT_STRIDE, 256 // HEATMAP_OUTPUT_STRIDE)  # (64, 64)\n",
    "SIGMA = 2  # Gaussian spread\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3fae71",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "69a1301b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform:\n",
    "    def __init__(self, size=(256, 256), augmentation=False):\n",
    "        self.size = size\n",
    "        if augmentation:\n",
    "            self.transform = T.Compose([\n",
    "                T.RandomHorizontalFlip(p=0.5),\n",
    "                T.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                T.Resize(size),\n",
    "                T.ToTensor(),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = T.Compose([\n",
    "                T.Resize(size),\n",
    "                T.ToTensor(),\n",
    "                # T.ToDtype(torch.float32, scale=True),\n",
    "                T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        orig_w, orig_h = image.size\n",
    "        image = self.transform(image)\n",
    "        \n",
    "        # Scale keypoints to new image size\n",
    "        annotations = []\n",
    "        for ann in target[\"annotations\"]:\n",
    "            kps = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "            kps[:, 0] = kps[:, 0] * (self.size[0] / orig_w)\n",
    "            kps[:, 1] = kps[:, 1] * (self.size[1] / orig_h)\n",
    "            ann['keypoints'] = kps.ravel().tolist()\n",
    "            annotations.append(ann)\n",
    "        \n",
    "        return image, {\n",
    "            \"image_id\": target[\"image_id\"],\n",
    "            \"annotations\": annotations,\n",
    "            \"orig_size\": target[\"orig_size\"]\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf63a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoKeypointsDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.coco = COCO(ann_file)\n",
    "        self._transforms = transforms\n",
    "        self.filter_without_keypoints = REMOVE_IMAGES_WITHOUT_KEYPOINTS\n",
    "        self.heatmap_size = HEATMAP_SIZE\n",
    "        self.output_stride = HEATMAP_OUTPUT_STRIDE\n",
    "        self.sigma = SIGMA\n",
    "\n",
    "        if self.filter_without_keypoints:\n",
    "            original_ids = list(self.ids)\n",
    "            self.ids = []\n",
    "            for img_id in original_ids:\n",
    "                ann_ids = self.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
    "                anns = self.coco.loadAnns(ann_ids)\n",
    "                anns_with_kp = [ann for ann in anns if 'keypoints' in ann and np.any(np.array(ann['keypoints']) != 0)]\n",
    "                if len(anns_with_kp) > 0:\n",
    "                    self.ids.append(img_id)\n",
    "\n",
    "        # cache annotations for each image\n",
    "        self.anns_per_image = {}\n",
    "        for img_id in self.ids:\n",
    "            ann_ids = self.coco.getAnnIds(imgIds=img_id)\n",
    "            anns = self.coco.loadAnns(ann_ids)\n",
    "            self.anns_per_image[img_id] = anns\n",
    "\n",
    "    def draw_gaussian(self, heatmap, center_x, center_y):\n",
    "        \"\"\"Draw a 2D Gaussian on heatmap channel (vectorized, correct region and shape)\"\"\"\n",
    "        height, width = heatmap.shape\n",
    "        sigma = self.sigma\n",
    "        \n",
    "        # bounds\n",
    "        x0 = int(max(0, center_x - 3 * sigma))\n",
    "        y0 = int(max(0, center_y - 3 * sigma))\n",
    "        x1 = int(min(width, center_x + 3 * sigma + 1))\n",
    "        y1 = int(min(height, center_y + 3 * sigma + 1))\n",
    "        if x1 <= x0 or y1 <= y0:\n",
    "            return\n",
    "        xs = np.arange(x0, x1)\n",
    "        ys = np.arange(y0, y1)\n",
    "        xx, yy = np.meshgrid(xs, ys, indexing='xy')  # xx,yy shape: (y1-y0, x1-x0)\n",
    "        d2 = (xx - center_x) ** 2 + (yy - center_y) ** 2\n",
    "        exponent = d2 / (2 * sigma ** 2)\n",
    "        mask = exponent <= 4.6052\n",
    "        g = np.exp(-exponent) * mask\n",
    "        patch = heatmap[y0:y1, x0:x1]\n",
    "        np.maximum(patch, g, out=patch)\n",
    "        heatmap[y0:y1, x0:x1] = patch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, _ = super().__getitem__(idx)\n",
    "        orig_w, orig_h = img.size\n",
    "        anns = self.anns_per_image[self.ids[idx]]\n",
    "\n",
    "        # Apply transforms first to get scaled keypoints\n",
    "        if self._transforms:\n",
    "            img, target = self._transforms(img, {\n",
    "                \"image_id\": self.ids[idx],\n",
    "                \"annotations\": anns,\n",
    "                \"orig_size\": (orig_w, orig_h)\n",
    "            })\n",
    "            anns = target[\"annotations\"]\n",
    "\n",
    "        # Ensure correct order: (width, height)\n",
    "        if hasattr(self._transforms, 'size'):\n",
    "            resized_w, resized_h = self._transforms.size\n",
    "        else:\n",
    "            resized_w, resized_h = (256, 256)\n",
    "        heatmap_h, heatmap_w = self.heatmap_size\n",
    "        \n",
    "        # print(f\"resized_w={resized_w}, resized_h={resized_h}, heatmap_w={heatmap_w}, heatmap_h={heatmap_h}\")\n",
    "        \n",
    "        # Create heatmap tensor: [NUM_KEYPOINTS, H, W]\n",
    "        heatmap = np.zeros((NUM_KEYPOINTS, heatmap_h, heatmap_w), dtype=np.float32)\n",
    "        for ann in anns:\n",
    "            kps = np.array(ann['keypoints']).reshape(-1, 3)\n",
    "            for kp_idx, (x, y, v) in enumerate(kps):\n",
    "                if v > 0:  # Only visible keypoints\n",
    "                    # Correct scaling: x to width, y to height\n",
    "                    x_hm = x * (heatmap_w / resized_w)\n",
    "                    y_hm = y * (heatmap_h / resized_h)\n",
    "                    self.draw_gaussian(heatmap[kp_idx], x_hm, y_hm)\n",
    "        return img, torch.from_numpy(heatmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3909ce8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=8.08s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=7.02s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.24s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CocoKeypointsDataset(\n",
    "    IMG_DIR_TRAIN, \n",
    "    ANN_FILE_TRAIN, \n",
    "    transforms=CustomTransform(augmentation=DATA_AUGMENTATION), \n",
    ")\n",
    "val_dataset = CocoKeypointsDataset(\n",
    "    IMG_DIR_VAL, \n",
    "    ANN_FILE_VAL, \n",
    "    transforms=CustomTransform(augmentation=False), \n",
    ")\n",
    "\n",
    "val_size = int(VAL_SPLIT * len(val_dataset))\n",
    "test_size = len(val_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(val_dataset, [val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1f565239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 56599\n",
      "val dataset size: 1173\n",
      "test dataset size: 1173\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset size:\", len(train_dataset))\n",
    "print(\"val dataset size:\", len(val_dataset))\n",
    "print(\"test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7548cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_len_train = int(TEST_VAL_TRAIN_PERCENT[0] * len(train_dataset))\n",
    "subset_len_val = int(TEST_VAL_TRAIN_PERCENT[1] * len(val_dataset))\n",
    "subset_len_test = int(TEST_VAL_TRAIN_PERCENT[2] * len(test_dataset))\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(train_dataset, range(subset_len_train))\n",
    "val_dataset = torch.utils.data.Subset(val_dataset, range(subset_len_val))\n",
    "test_dataset = torch.utils.data.Subset(test_dataset, range(subset_len_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e9606e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    pin_memory=True,            # faster GPU transfer\n",
    "    # persistent_workers=True     # maintain worker pool\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    pin_memory=True,\n",
    "    # persistent_workers=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False, \n",
    "    pin_memory=True,\n",
    "    # persistent_workers=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3dc9bc89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size: 5659\n",
      "val dataset size: 117\n",
      "test dataset size: 117\n"
     ]
    }
   ],
   "source": [
    "print(\"train dataset size:\", len(train_dataset))\n",
    "print(\"val dataset size:\", len(val_dataset))\n",
    "print(\"test dataset size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e28b54",
   "metadata": {},
   "source": [
    "## Visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "36cc31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(img_tensor):\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return img_tensor * std + mean\n",
    "\n",
    "def visualize_heatmaps(image, gt_heatmap, pred_heatmap=None, keypoint_idx=0):\n",
    "    \"\"\"Visualize input image with ground truth and predicted heatmaps\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3 if pred_heatmap is not None else 2, figsize=(15, 5))\n",
    "    \n",
    "    # Original image\n",
    "    img = unnormalize(image).permute(1, 2, 0).cpu().numpy()\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title('Input Image')\n",
    "    axes[0].axis('off')\n",
    "    \n",
    "    # Ground truth heatmap\n",
    "    gt_hm = gt_heatmap[keypoint_idx].cpu().numpy()\n",
    "    axes[1].imshow(gt_hm, cmap='jet', alpha=0.5)\n",
    "    axes[1].set_title(f'GT Heatmap (Keypoint {KEYPOINT_NAMES[keypoint_idx]})')\n",
    "    axes[1].axis('off')\n",
    "    \n",
    "    # Predicted heatmap (if available)\n",
    "    if pred_heatmap is not None:\n",
    "        pred_hm = pred_heatmap[keypoint_idx].detach().cpu().numpy()\n",
    "        axes[2].imshow(pred_hm, cmap='jet', alpha=0.5)\n",
    "        axes[2].set_title(f'Predicted Heatmap (Keypoint {KEYPOINT_NAMES[keypoint_idx]})')\n",
    "        axes[2].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12f9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "images, gt_heatmaps = next(iter(val_loader))\n",
    "images = images.to(device)\n",
    "gt_heatmaps = gt_heatmaps.to(device)\n",
    "\n",
    "idx = random.randint(0, images.size(0)-1)\n",
    "\n",
    "for kpidx in range(0, NUM_KEYPOINTS):\n",
    "    visualize_heatmaps(\n",
    "        images[idx].cpu(),\n",
    "        gt_heatmaps[idx].cpu(),\n",
    "        keypoint_idx = kpidx\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1949b8fa",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "492942b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeatmapModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        \n",
    "        # Remove the last two layers (avgpool and fc)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        \n",
    "        # Upsampling layers to increase resolution\n",
    "        self.upsample = nn.Sequential(\n",
    "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, NUM_KEYPOINTS, kernel_size=1)\n",
    "        )\n",
    "        \n",
    "        # Initialize last layer\n",
    "        nn.init.normal_(self.upsample[-1].weight, std=0.001)\n",
    "        nn.init.constant_(self.upsample[-1].bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)  # [B, 512, 8, 8]\n",
    "        x = self.upsample(x)  # [B, 17, 64, 64]\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1391d205",
   "metadata": {},
   "source": [
    "# Model training functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff4ee78",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "59e8f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_loss(pred_heatmaps, gt_heatmaps):\n",
    "    \"\"\"MSE loss with emphasis on positive pixels\"\"\"\n",
    "    # Basic MSE loss\n",
    "    loss = F.mse_loss(pred_heatmaps, gt_heatmaps, reduction='none')\n",
    "    \n",
    "    # Increase weight for positive pixels\n",
    "    pos_mask = gt_heatmaps > 0.1\n",
    "    loss[pos_mask] *= 3.0\n",
    "    \n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e955acf3",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e75f181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "72da4027",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = HeatmapModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f4a7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SharedData\\Documents\\GitHub\\NN-Project\\temp\\temp\\wandb\\run-20250615_140918-9ahdl705</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fejowo5522-/NN_Project/runs/9ahdl705' target=\"_blank\">lucky-lion-28</a></strong> to <a href='https://wandb.ai/fejowo5522-/NN_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fejowo5522-/NN_Project' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fejowo5522-/NN_Project/runs/9ahdl705' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project/runs/9ahdl705</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fejowo5522-/NN_Project/runs/9ahdl705?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x2a28bf16ea0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_config = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_size\": subset_len_train,\n",
    "    \"val_size\": subset_len_val,\n",
    "    \"test_size\": subset_len_test,\n",
    "    \"model_name\": \"HeatmapResNet\",\n",
    "    \"criterion\": \"mse\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"num_keypoints\": NUM_KEYPOINTS,\n",
    "    \"remove_images_without_keypoints\": REMOVE_IMAGES_WITHOUT_KEYPOINTS,\n",
    "    \"val_split\": VAL_SPLIT,\n",
    "    \"test_val_train_percent\": TEST_VAL_TRAIN_PERCENT,\n",
    "    \"device\": device,\n",
    "    \"data_augmentation\": DATA_AUGMENTATION,\n",
    "    \"heatmap_stride\": HEATMAP_OUTPUT_STRIDE,\n",
    "    \"heatmap_size\": HEATMAP_SIZE,\n",
    "    \"heatmap_sigma\": SIGMA,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"fejowo5522-\",\n",
    "    project=\"NN_Project\",\n",
    "    config=wandb_config,\n",
    "    group=\"KeypointDetectionHeatmap\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a2d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = True\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for images, heatmaps in tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}'):\n",
    "        images = images.to(device)\n",
    "        heatmaps = heatmaps.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        pred_heatmaps = model(images)\n",
    "        loss = heatmap_loss(pred_heatmaps, heatmaps)\n",
    "        \n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation loss calculation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_images, val_heatmaps in val_loader:\n",
    "            val_images = val_images.to(device)\n",
    "            val_heatmaps = val_heatmaps.to(device)\n",
    "            val_pred_heatmaps = model(val_images)\n",
    "            loss = heatmap_loss(val_pred_heatmaps, val_heatmaps)\n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': avg_train_loss,\n",
    "        'val_loss': avg_val_loss\n",
    "    })\n",
    "\n",
    "    print(f'Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f} Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "\n",
    "    if early_stopping:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"temp_best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f0d928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇█████</td></tr><tr><td>test_loss</td><td>▁</td></tr><tr><td>train_loss</td><td>█▇▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>██▇▅▅▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>73</td></tr><tr><td>test_loss</td><td>2418.70361</td></tr><tr><td>train_loss</td><td>651.28759</td></tr><tr><td>val_loss</td><td>3612.5603</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">lucky-lion-28</strong> at: <a href='https://wandb.ai/fejowo5522-/NN_Project/runs/9ahdl705' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project/runs/9ahdl705</a><br> View project at: <a href='https://wandb.ai/fejowo5522-/NN_Project' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250615_140918-9ahdl705\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "# Load best model if early stopping was used\n",
    "if early_stopping and os.path.exists(\"temp_best_model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"temp_best_model.pth\"))\n",
    "\n",
    "# Run model on all test data and collect predictions and ground truths\n",
    "model.eval()\n",
    "preds_list = []\n",
    "gt_list = []\n",
    "with torch.no_grad():\n",
    "    for images, gt_heatmaps in test_loader:\n",
    "        images = images.to(device)\n",
    "        gt_heatmaps = gt_heatmaps.to(device)\n",
    "        pred_heatmaps = model(images)\n",
    "        preds_list.append(pred_heatmaps.cpu())\n",
    "        gt_list.append(gt_heatmaps.cpu())\n",
    "preds_all = torch.cat(preds_list, dim=0)\n",
    "gt_all = torch.cat(gt_list, dim=0)\n",
    "test_loss = heatmap_loss(preds_all, gt_all).item()\n",
    "\n",
    "wandb.log({\n",
    "    'test_loss': test_loss\n",
    "})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80c04696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to keypoint_model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"keypoint_model.pth\")\n",
    "print(\"Model saved to keypoint_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d420164",
   "metadata": {},
   "source": [
    "# Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5859b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, visualize predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    images, gt_heatmaps = next(iter(val_loader))\n",
    "    images = images.to(device)\n",
    "    gt_heatmaps = gt_heatmaps.to(device)\n",
    "    pred_heatmaps = model(images)\n",
    "    \n",
    "    # Select random sample from batch\n",
    "    idx = random.randint(0, images.size(0)-1)\n",
    "    for kpidx in range(0, NUM_KEYPOINTS):\n",
    "        visualize_heatmaps(\n",
    "            images[idx].cpu(),\n",
    "            gt_heatmaps[idx].cpu(),\n",
    "            pred_heatmaps[idx].cpu(),\n",
    "            keypoint_idx = kpidx\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
