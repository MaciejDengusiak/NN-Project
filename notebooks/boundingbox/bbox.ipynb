{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b82c9c1",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "735600f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CocoDetection\n",
    "from pycocotools.coco import COCO\n",
    "import torchvision.transforms as T\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms.functional as VF\n",
    "from torch.nn import functional as F\n",
    "import wandb\n",
    "from itertools import cycle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b7268e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "COCO_PATH = \"../../data/coco/\"\n",
    "IMG_DIR_TRAIN = os.path.join(COCO_PATH, \"images/train2017\")\n",
    "IMG_DIR_VAL = os.path.join(COCO_PATH, \"images/val2017\")\n",
    "ANN_FILE_TRAIN = os.path.join(COCO_PATH, \"annotations/instances_train2017.json\")\n",
    "ANN_FILE_VAL = os.path.join(COCO_PATH, \"annotations/instances_val2017.json\")\n",
    "\n",
    "GRID_ROWS = 6\n",
    "GRID_COLS = 6\n",
    "MAX_BOXES_PER_CELL = 2\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (256, 256)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "DS_PERCENT = (0.1, 0.1, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98ff7bf",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "875da2b8",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0b7a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransform:\n",
    "    def __init__(self, size=IMAGE_SIZE):\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize(size),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, image, target):\n",
    "        return self.transform(image), target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3bc0d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CocoBBoxDataset(CocoDetection):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.coco = COCO(ann_file)\n",
    "        self._transforms = transforms\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img, target = super().__getitem__(idx)\n",
    "        orig_w, orig_h = img.size\n",
    "        \n",
    "        anns = []\n",
    "        for obj in target:\n",
    "            if obj['category_id'] == 1:  # Only humans\n",
    "                anns.append(obj)\n",
    "                \n",
    "        if self._transforms:\n",
    "            img, _ = self._transforms(img, None)\n",
    "            \n",
    "        # Create target tensor [GRID_ROWS, GRID_COLS, MAX_BOXES_PER_CELL, 5]\n",
    "        target_tensor = torch.zeros(GRID_ROWS, GRID_COLS, MAX_BOXES_PER_CELL, 5)\n",
    "        \n",
    "        for ann in anns:\n",
    "            bbox = ann['bbox']  # [x, y, w, h]\n",
    "            # Scale to image size\n",
    "            x = bbox[0] * (IMAGE_SIZE[0] / orig_w)\n",
    "            y = bbox[1] * (IMAGE_SIZE[1] / orig_h)\n",
    "            w = bbox[2] * (IMAGE_SIZE[0] / orig_w)\n",
    "            h = bbox[3] * (IMAGE_SIZE[1] / orig_h)\n",
    "            \n",
    "            # Convert to center coordinates\n",
    "            x_center = x + w/2\n",
    "            y_center = y + h/2\n",
    "            \n",
    "            # Find grid cell\n",
    "            cell_i = min(int(y_center * GRID_ROWS / IMAGE_SIZE[1]), GRID_ROWS-1)\n",
    "            cell_j = min(int(x_center * GRID_COLS / IMAGE_SIZE[0]), GRID_COLS-1)\n",
    "            \n",
    "            # Normalize to cell\n",
    "            cell_w = IMAGE_SIZE[0] / GRID_COLS\n",
    "            cell_h = IMAGE_SIZE[1] / GRID_ROWS\n",
    "            x_cell = (x_center - cell_j * cell_w) / cell_w\n",
    "            y_cell = (y_center - cell_i * cell_h) / cell_h\n",
    "            w_norm = w / IMAGE_SIZE[0]\n",
    "            h_norm = h / IMAGE_SIZE[1]\n",
    "            \n",
    "            # Find empty slot\n",
    "            for k in range(MAX_BOXES_PER_CELL):\n",
    "                if target_tensor[cell_i, cell_j, k, 4] == 0:  # Check if slot is empty\n",
    "                    target_tensor[cell_i, cell_j, k] = torch.tensor([x_cell, y_cell, w_norm, h_norm, 1.0])\n",
    "                    break\n",
    "                    \n",
    "        return img, target_tensor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fc329f",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6cf1257e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"BBoxModelSimple\"\n",
    "\n",
    "class BBoxModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((GRID_ROWS, GRID_COLS))\n",
    "        self.conv_head = nn.Conv2d(512, MAX_BOXES_PER_CELL * 5, kernel_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = self.conv_head(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = x.view(x.size(0), GRID_ROWS, GRID_COLS, MAX_BOXES_PER_CELL, 5)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285efb95",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e69d9c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "CRITERION_NAME = \"mse+bce\"\n",
    "\n",
    "def bbox_loss(pred, target, lambda_coord=5.0):\n",
    "    # Extract components\n",
    "    pred_xy = pred[..., :2]\n",
    "    pred_wh = pred[..., 2:4]\n",
    "    pred_conf = pred[..., 4]\n",
    "    \n",
    "    target_xy = target[..., :2]\n",
    "    target_wh = target[..., 2:4]\n",
    "    target_conf = target[..., 4]\n",
    "    \n",
    "    # Coordinate loss (only for boxes with objects)\n",
    "    obj_mask = target_conf == 1\n",
    "    if obj_mask.sum() > 0:\n",
    "        xy_loss = nn.functional.mse_loss(\n",
    "            pred_xy[obj_mask], \n",
    "            target_xy[obj_mask],\n",
    "            reduction='sum'\n",
    "        )\n",
    "        wh_loss = nn.functional.mse_loss(\n",
    "            pred_wh[obj_mask],\n",
    "            target_wh[obj_mask],\n",
    "            reduction='sum'\n",
    "        )\n",
    "        coord_loss = (xy_loss + wh_loss) * lambda_coord\n",
    "    else:\n",
    "        coord_loss = torch.tensor(0.0, device=pred.device)\n",
    "    \n",
    "\n",
    "    # Confidence loss (BCE)\n",
    "    conf_loss = nn.functional.binary_cross_entropy_with_logits(\n",
    "        pred_conf, target_conf, reduction='sum'\n",
    "    )\n",
    "\n",
    "\n",
    "    total_loss = (conf_loss + coord_loss) / BATCH_SIZE\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0389fe45",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "805eff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, verbose_tqdm=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dl = tqdm(dataloader, desc=\"Training\") if verbose_tqdm else dataloader    \n",
    "    for imgs, keypoints in dl:\n",
    "        imgs = imgs.to(device)\n",
    "        keypoints = keypoints.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, keypoints)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, verbose_tqdm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dl = tqdm(dataloader, desc=\"Evaluating\") if verbose_tqdm else dataloader\n",
    "    with torch.no_grad():\n",
    "        for imgs, keypoints in dl:\n",
    "            imgs = imgs.to(device)\n",
    "            keypoints = keypoints.to(device)\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, keypoints)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3950da",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc9dd7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_bbox(img_tensor, target_bbox, pred_bbox=None, conf_threshold=0.5):\n",
    "    img = img_tensor.cpu().permute(1, 2, 0).numpy()\n",
    "    img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
    "    img = np.clip(img, 0, 1)\n",
    "    \n",
    "    fig, ax = plt.subplots(1, figsize=(10, 10))\n",
    "    ax.imshow(img)\n",
    "    \n",
    "    # Draw grid\n",
    "    for i in range(1, GRID_ROWS):\n",
    "        ax.axhline(y=i * IMAGE_SIZE[1] / GRID_ROWS, color='r', linestyle='-', alpha=0.3)\n",
    "    for j in range(1, GRID_COLS):\n",
    "        ax.axvline(x=j * IMAGE_SIZE[0] / GRID_COLS, color='r', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Draw ground truth boxes\n",
    "    for i in range(GRID_ROWS):\n",
    "        for j in range(GRID_COLS):\n",
    "            for k in range(MAX_BOXES_PER_CELL):\n",
    "                if target_bbox[i, j, k, 4] == 1:\n",
    "                    x_cell, y_cell, w_norm, h_norm = target_bbox[i, j, k, :4]\n",
    "                    \n",
    "                    cell_w = IMAGE_SIZE[0] / GRID_COLS\n",
    "                    cell_h = IMAGE_SIZE[1] / GRID_ROWS\n",
    "                    \n",
    "                    x_center = j * cell_w + x_cell * cell_w\n",
    "                    y_center = i * cell_h + y_cell * cell_h\n",
    "                    width = w_norm * IMAGE_SIZE[0]\n",
    "                    height = h_norm * IMAGE_SIZE[1]\n",
    "                    \n",
    "                    x = x_center - width / 2\n",
    "                    y = y_center - height / 2\n",
    "                    \n",
    "                    rect = patches.Rectangle(\n",
    "                        (x, y), width, height, \n",
    "                        linewidth=2, edgecolor='g', facecolor='none'\n",
    "                    )\n",
    "                    ax.add_patch(rect)\n",
    "    \n",
    "    # Draw predicted boxes\n",
    "    if pred_bbox is not None:\n",
    "        pred_conf = torch.sigmoid(pred_bbox[..., 4])\n",
    "        pred_xy = torch.sigmoid(pred_bbox[..., :2])\n",
    "        pred_wh = torch.sigmoid(pred_bbox[..., 2:4])\n",
    "        \n",
    "        for i in range(GRID_ROWS):\n",
    "            for j in range(GRID_COLS):\n",
    "                for k in range(MAX_BOXES_PER_CELL):\n",
    "                    if pred_conf[i, j, k] > conf_threshold:\n",
    "                        x_cell, y_cell = pred_xy[i, j, k]\n",
    "                        w_norm, h_norm = pred_wh[i, j, k]\n",
    "                        \n",
    "                        cell_w = IMAGE_SIZE[0] / GRID_COLS\n",
    "                        cell_h = IMAGE_SIZE[1] / GRID_ROWS\n",
    "                        \n",
    "                        x_center = j * cell_w + x_cell * cell_w\n",
    "                        y_center = i * cell_h + y_cell * cell_h\n",
    "                        width = w_norm * IMAGE_SIZE[0]\n",
    "                        height = h_norm * IMAGE_SIZE[1]\n",
    "                        \n",
    "                        x = x_center - width / 2\n",
    "                        y = y_center - height / 2\n",
    "                        \n",
    "                        rect = patches.Rectangle(\n",
    "                            (x, y), width, height, \n",
    "                            linewidth=2, edgecolor='r', facecolor='none', linestyle='--'\n",
    "                        )\n",
    "                        ax.add_patch(rect)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0285488b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831928aa",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2b73db54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=12.13s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=12.14s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.51s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.48s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Subset\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CocoBBoxDataset(IMG_DIR_TRAIN, ANN_FILE_TRAIN, transforms=CustomTransform())\n",
    "val_dataset = CocoBBoxDataset(IMG_DIR_VAL, ANN_FILE_VAL, transforms=CustomTransform())\n",
    "\n",
    "# Split val_dataset into validation and test sets\n",
    "val_size = len(val_dataset)\n",
    "test_split = 0.5  # 50% for validation, 50% for test (adjust as needed)\n",
    "val_indices = list(range(val_size))\n",
    "split = int(np.floor(test_split * val_size))\n",
    "np.random.shuffle(val_indices)\n",
    "val_idx, test_idx = val_indices[split:], val_indices[:split]\n",
    "\n",
    "\n",
    "val_subset = Subset(val_dataset, val_idx)\n",
    "test_subset = Subset(val_dataset, test_idx)\n",
    "\n",
    "# Apply DS_PERCENT to subsample the datasets\n",
    "train_percent, val_percent, test_percent = DS_PERCENT\n",
    "\n",
    "if train_percent < 1.0:\n",
    "    train_len = int(len(train_dataset) * train_percent)\n",
    "    train_indices = np.random.choice(len(train_dataset), train_len, replace=False)\n",
    "    train_indices = [int(i) for i in train_indices]\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "\n",
    "if val_percent < 1.0:\n",
    "    val_len = int(len(val_subset) * val_percent)\n",
    "    val_indices_sub = np.random.choice(len(val_subset), val_len, replace=False)\n",
    "    val_indices_sub = [int(i) for i in val_indices_sub]\n",
    "    val_subset = Subset(val_subset, val_indices_sub)\n",
    "\n",
    "if test_percent < 1.0:\n",
    "    test_len = int(len(test_subset) * test_percent)\n",
    "    test_indices_sub = np.random.choice(len(test_subset), test_len, replace=False)\n",
    "    test_indices_sub = [int(i) for i in test_indices_sub]\n",
    "    test_subset = Subset(test_subset, test_indices_sub)\n",
    "\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_subset, batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "575f9f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset length: 11828\n",
      "Validation dataset length: 250\n",
      "Test dataset length: 250\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset length:\", len(train_dataset))\n",
    "print(\"Validation dataset length:\", len(val_subset))\n",
    "print(\"Test dataset length:\", len(test_subset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b09836",
   "metadata": {},
   "source": [
    "## GT visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dc17e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random validation image with only ground truth boxes\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "img, target = val_dataset[idx]\n",
    "visualize_bbox(img, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c4330",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7c67d192",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f7216fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = BBoxModel().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "criterion = bbox_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9037709b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">young-snow-66</strong> at: <a href='https://wandb.ai/fejowo5522-/NN_Project/runs/6uw7ndv3' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project/runs/6uw7ndv3</a><br> View project at: <a href='https://wandb.ai/fejowo5522-/NN_Project' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250622_100414-6uw7ndv3\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\SharedData\\Documents\\GitHub\\NN-Project\\notebooks\\boundingbox\\wandb\\run-20250622_100552-97inemcc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/fejowo5522-/NN_Project/runs/97inemcc' target=\"_blank\">dauntless-wave-67</a></strong> to <a href='https://wandb.ai/fejowo5522-/NN_Project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/fejowo5522-/NN_Project' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/fejowo5522-/NN_Project/runs/97inemcc' target=\"_blank\">https://wandb.ai/fejowo5522-/NN_Project/runs/97inemcc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/fejowo5522-/NN_Project/runs/97inemcc?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x1b285705be0>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb_config = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_size\": len(train_dataset),\n",
    "    \"val_size\": len(val_subset),\n",
    "    \"test_size\": len(test_subset),\n",
    "    \"grid_rows\": GRID_ROWS,\n",
    "    \"grid_cols\": GRID_COLS,\n",
    "    \"max_boxes_per_cell\": MAX_BOXES_PER_CELL,\n",
    "    \"device\": device,\n",
    "    \"image_size\": IMAGE_SIZE,    \n",
    "    \"criterion\": CRITERION_NAME,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"model_name\": MODEL_NAME,\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"fejowo5522-\",\n",
    "    project=\"NN_Project\",\n",
    "    config=wandb_config,\n",
    "    group=\"BBoxDetection\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0357e77d",
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_tqdm = True\n",
    "\n",
    "early_stopping = True\n",
    "patience = 10\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3be1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316a0e22595a47a59bf30e5fb70913de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f9908bd64714a80a1232824adc322bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/370 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, verbose_tqdm=verbose_tqdm)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, val_loader, criterion, verbose_tqdm=verbose_tqdm)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a84bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "\n",
    "# Evaluate on test set\n",
    "model.eval()\n",
    "test_loss = evaluate(model, test_loader, criterion)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Log test results\n",
    "wandb.log({'test_loss': test_loss})\n",
    "wandb.finish()\n",
    "\n",
    "# Save final model\n",
    "torch.save(model.state_dict(), \"bbox_model.pth\")\n",
    "print(\"Model saved to bbox_model.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5349fe74",
   "metadata": {},
   "source": [
    "## Result evaluation and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a7a3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "img, target = next(iter(val_loader))\n",
    "with torch.no_grad():\n",
    "    pred = model(img.to(device))[0].cpu()\n",
    "    \n",
    "visualize_bbox(img[0], target[0], pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
