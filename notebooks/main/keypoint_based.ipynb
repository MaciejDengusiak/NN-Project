{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torchvision.transforms import functional as F\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms.functional as VF\n",
    "# from pycocotools.coco import COCO\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaciejdengusiak\u001b[0m (\u001b[33mfejowo5522-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(device)\n",
    "\n",
    "# reduce cpu contention\n",
    "torch.set_num_threads(1)\n",
    "NUM_WORKERS = 6  # adjust based on CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_KEYPOINTS = 17\n",
    "\n",
    "NUM_CLASSES = 51\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "CROP_SIZE = (256, 256)\n",
    "DATA_AUGMENTATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, verbose_tqdm=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dl = tqdm(dataloader, desc=\"Training\") if verbose_tqdm else dataloader    \n",
    "    for imgs, keypoints in dl:\n",
    "        imgs = imgs.to(device)\n",
    "        keypoints = keypoints.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, keypoints)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, verbose_tqdm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dl = tqdm(dataloader, desc=\"Evaluating\") if verbose_tqdm else dataloader\n",
    "    with torch.no_grad():\n",
    "        for imgs, keypoints in dl:\n",
    "            imgs = imgs.to(device)\n",
    "            keypoints = keypoints.to(device)\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, keypoints)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, keypoint_model, device):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.keypoint_model = keypoint_model.eval().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.base_dataset[idx]  # frames: (16, 3, 224, 224)\n",
    "        frames = frames.to(self.device)\n",
    "\n",
    "        keypoints_seq = []\n",
    "        with torch.no_grad():\n",
    "            for frame in frames:\n",
    "                keypoints = self.keypoint_model(frame.unsqueeze(0))  # (1, 3, 224, 224)\n",
    "                keypoints_seq.append(keypoints.squeeze(0))  # (17, 2)\n",
    "\n",
    "        keypoints_tensor = torch.stack(keypoints_seq)  # shape: (16, 17, 2)\n",
    "\n",
    "        return (frames.cpu(), keypoints_tensor.cpu()), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActionsFusionModel(nn.Module):\n",
    "    def __init__(self, num_keypoints=NUM_KEYPOINTS, num_actions=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # === Wizualny tor (CNN) ===\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.cnn_backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.feature_dim_img = base_model.fc.in_features  # 512\n",
    "\n",
    "        for param in self.cnn_backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # === Tor dla keypointów === \n",
    "        self.keypoint_dim = num_keypoints * 2  # (x,y) dla każdego punktu\n",
    "\n",
    "        self.keypoint_mlp = nn.Sequential(\n",
    "            nn.Linear(self.keypoint_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # === Klasyfikator na podstawie fuzji ===\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim_img + 128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_kp):\n",
    "        # === Obrazy ===\n",
    "        B, T, C, H, W = x_img.shape\n",
    "        x_img = x_img.view(B * T, C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat_img = self.cnn_backbone(x_img)  # (B*T, 512, 1, 1)\n",
    "        feat_img = feat_img.view(B, T, self.feature_dim_img)\n",
    "        feat_img = feat_img.mean(dim=1)  # (B, 512)\n",
    "\n",
    "        # === Keypointy ===\n",
    "        B, T, N, _ = x_kp.shape\n",
    "        x_kp = x_kp.view(B, T, -1)           # (B, T, N*2)\n",
    "        feat_kp = self.keypoint_mlp(x_kp)    # (B, T, 128)\n",
    "        feat_kp = feat_kp.mean(dim=1)        # (B, 128)\n",
    "\n",
    "        # === Fuzja ===\n",
    "        fused = torch.cat([feat_img, feat_kp], dim=1)  # (B, 640)\n",
    "\n",
    "        out = self.classifier(fused)  # (B, num_actions)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class KeypointCropModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = resnet18(pretrained=True)\n",
    "        self.backbone = nn.Sequential(*list(resnet.children())[:-2])  # Remove avgpool and fc\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, NUM_KEYPOINTS * 3)  # Predict x, y, confidence for each keypoint\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, NUM_KEYPOINTS, 3)\n",
    "        return x\n",
    "\n",
    "class PersonKeypointPipeline:\n",
    "    def __init__(self, detector, keypoint_model, crop_transform, device='cpu', detection_threshold=0.8, crop_size=256):\n",
    "        self.detector = detector.to(device)\n",
    "        self.detector.eval()\n",
    "        self.keypoint_model = keypoint_model.to(device)\n",
    "        self.keypoint_model.eval()\n",
    "        self.crop_transform = crop_transform\n",
    "        self.device = device\n",
    "        self.detection_threshold = detection_threshold\n",
    "        self.crop_size = crop_size\n",
    "\n",
    "    def predict(self, pil_img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pil_img: PIL.Image.Image, RGB image\n",
    "\n",
    "        Returns:\n",
    "            List of dicts, each with:\n",
    "                - 'box': [x1, y1, x2, y2]\n",
    "                - 'keypoints': np.ndarray of shape (NUM_KEYPOINTS, 3)\n",
    "        \"\"\"\n",
    "        # Detect people\n",
    "        img_tensor = T.Compose([T.ToTensor()])(pil_img).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            detections = self.detector(img_tensor)[0]\n",
    "\n",
    "        person_mask = (detections['labels'] == 1) & (detections['scores'] > self.detection_threshold)\n",
    "        boxes = detections['boxes'][person_mask].cpu().numpy()\n",
    "\n",
    "        results = []\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            crop = pil_img.crop((x1, y1, x2, y2)).resize((self.crop_size, self.crop_size))\n",
    "            crop_tensor = self.crop_transform(crop).unsqueeze(0).to(self.device)\n",
    "            with torch.no_grad():\n",
    "                pred_kps = self.keypoint_model(crop_tensor)[0].cpu()\n",
    "            pred_kps[:, 2] = torch.sigmoid(pred_kps[:, 2])\n",
    "\n",
    "            # Map keypoints back to original image coordinates\n",
    "            box_w, box_h = x2 - x1, y2 - y1\n",
    "            mapped_kps = []\n",
    "            for kp in pred_kps:\n",
    "                orig_x = kp[0] * (box_w / self.crop_size) + x1\n",
    "                orig_y = kp[1] * (box_h / self.crop_size) + y1\n",
    "                visible = 1 if kp[2] > 0.5 else 0\n",
    "                mapped_kps.append([orig_x.item(), orig_y.item(), visible])\n",
    "            mapped_kps = np.array(mapped_kps)\n",
    "\n",
    "            results.append({'box': [x1, y1, x2, y2], 'keypoints': mapped_kps})\n",
    "\n",
    "        return results\n",
    "    \n",
    "class CropTransform:\n",
    "    def __init__(self, augmentation=False):\n",
    "        if augmentation:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
    "                transforms.RandomApply([transforms.RandomGrayscale(p=1.0)], p=0.2),\n",
    "                transforms.RandomAdjustSharpness(sharpness_factor=2, p=0.3),\n",
    "                transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ])\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return self.transform(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import actions.data_loader as dl\n",
    "import keypoints.keypoints_boundingbox_approach as kp\n",
    "\n",
    "\n",
    "\n",
    "loaded_pipeline = KeypointCropModel().to(device)\n",
    "loaded_pipeline = torch.load(\"../../models/keypoints_model_pipeline.pth\", map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = KeypointCropModel().to(device)\n",
    "model.load_state_dict(torch.load(\"../../models/bb_23loss_keypoint_crop_model.pth\", map_location=device))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "img_tensor = dl.dataset_train[0][0][0]\n",
    "\n",
    "\n",
    "# preds = loa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.13/site-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# x = dl.dataset_train[0][0][0]\n",
    "\n",
    "\n",
    "# predictions = loaded_pipeline.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_batch(model, imgs):\n",
    "    \"\"\"\n",
    "    imgs: (B, T, 3, H, W)\n",
    "    Zwraca: (B, T, N, 2) – keypointy\n",
    "    \"\"\"\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    keypoints_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        sample_keypoints = []\n",
    "        for t in range(T):\n",
    "            frame = imgs[b, t].unsqueeze(0)  # (1, 3, H, W)\n",
    "            kp = model(frame)  # np. (1, N, 2)\n",
    "            sample_keypoints.append(kp.squeeze(0).cpu())  # (N, 2)\n",
    "        sample_keypoints = torch.stack(sample_keypoints, dim=0)  # (T, N, 2)\n",
    "        keypoints_list.append(sample_keypoints)\n",
    "\n",
    "    keypoints_tensor = torch.stack(keypoints_list, dim=0)  # (B, T, N, 2)\n",
    "    return keypoints_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionWrapperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, keypoint_model):\n",
    "        self.base = base_dataset\n",
    "        self.keypoint_model = keypoint_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs, label = self.base[idx]  # imgs: (T, 3, H, W)\n",
    "        imgs = imgs.unsqueeze(0)  # (1, T, 3, H, W)\n",
    "\n",
    "        keypoints = extract_keypoints_batch(self.keypoint_model, imgs)  # (1, T, N, 2)\n",
    "        keypoints = keypoints.squeeze(0)  # (T, N, 2)\n",
    "\n",
    "        return imgs.squeeze(0), keypoints, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ActionsFusionModel.__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m test_loader_fused  = DataLoader(fusion_test, batch_size=\u001b[32m8\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mActionsFusionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_keypoints\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m51\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n",
      "\u001b[31mTypeError\u001b[39m: ActionsFusionModel.__init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "fusion_train = FusionWrapperDataset(dl.dataset_train, keypoint_model)\n",
    "fusion_val   = FusionWrapperDataset(dl.dataset_valid, keypoint_model)\n",
    "fusion_test  = FusionWrapperDataset(dl.dataset_test, keypoint_model)\n",
    "\n",
    "train_loader_fused = DataLoader(fusion_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_fused   = DataLoader(fusion_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader_fused  = DataLoader(fusion_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = ActionsFusionModel(num_keypoints=17, num_classes=51).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss();\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_size\": len(fusion_train),\n",
    "    \"val_size\": len(fusion_val),\n",
    "    \"test_size\": len(fusion_test),\n",
    "    \"model\": \"ActionsBaselineModel\",\n",
    "    \"criterion\": \"Cross entropy loss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"crop_size\": CROP_SIZE,\n",
    "    \"device\": device,\n",
    "    \"data_augmentation\": DATA_AUGMENTATION\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"fejowo5522-\",\n",
    "    project=\"NN_Project\",\n",
    "    config=wandb_config,\n",
    "    group=\"ActionsBaseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_tqdm = True\n",
    "early_stopping = True\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, dl.train_loader, optimizer, criterion, verbose_tqdm=verbose_tqdm)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, dl.val_loader, criterion, verbose_tqdm=verbose_tqdm)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
