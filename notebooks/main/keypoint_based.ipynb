{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "# from torchvision.transforms import functional as F\n",
    "from torch.nn import functional as F\n",
    "import torchvision.transforms.functional as VF\n",
    "# from pycocotools.coco import COCO\n",
    "import torchvision.transforms.v2 as T\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18\n",
    "from torchvision import transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import cycle\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torchvision.models as models\n",
    "\n",
    "import wandb\n",
    "\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(device)\n",
    "\n",
    "# reduce cpu contention\n",
    "torch.set_num_threads(1)\n",
    "NUM_WORKERS = 6  # adjust based on CPU cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 51\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "CROP_SIZE = (256, 256)\n",
    "DATA_AUGMENTATION = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, verbose_tqdm=False):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dl = tqdm(dataloader, desc=\"Training\") if verbose_tqdm else dataloader    \n",
    "    for imgs, keypoints in dl:\n",
    "        imgs = imgs.to(device)\n",
    "        keypoints = keypoints.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, keypoints)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, verbose_tqdm=False):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "\n",
    "    dl = tqdm(dataloader, desc=\"Evaluating\") if verbose_tqdm else dataloader\n",
    "    with torch.no_grad():\n",
    "        for imgs, keypoints in dl:\n",
    "            imgs = imgs.to(device)\n",
    "            keypoints = keypoints.to(device)\n",
    "            \n",
    "            preds = model(imgs)\n",
    "            loss = criterion(preds, keypoints)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, keypoint_model, device):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.keypoint_model = keypoint_model.eval().to(device)\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frames, label = self.base_dataset[idx]  # frames: (16, 3, 224, 224)\n",
    "        frames = frames.to(self.device)\n",
    "\n",
    "        keypoints_seq = []\n",
    "        with torch.no_grad():\n",
    "            for frame in frames:\n",
    "                keypoints = self.keypoint_model(frame.unsqueeze(0))  # (1, 3, 224, 224)\n",
    "                keypoints_seq.append(keypoints.squeeze(0))  # (17, 2)\n",
    "\n",
    "        keypoints_tensor = torch.stack(keypoints_seq)  # shape: (16, 17, 2)\n",
    "\n",
    "        return (frames.cpu(), keypoints_tensor.cpu()), label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ActionsFusionModel(nn.Module):\n",
    "    def __init__(self, num_keypoints=17, num_actions=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # === Wizualny tor (CNN) ===\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.cnn_backbone = nn.Sequential(*list(base_model.children())[:-1])\n",
    "        self.feature_dim_img = base_model.fc.in_features  # 512\n",
    "\n",
    "        for param in self.cnn_backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # === Tor dla keypointów === TODO: Tu trzeba jeszce ogarnąć jakie jest wejście - keypoint_dim\n",
    "        self.keypoint_dim = num_keypoints * 2  # (x,y) dla każdego punktu\n",
    "\n",
    "        self.keypoint_mlp = nn.Sequential(\n",
    "            nn.Linear(self.keypoint_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # === Klasyfikator na podstawie fuzji ===\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.feature_dim_img + 128, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(256, num_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x_img, x_kp):\n",
    "        # === Obrazy ===\n",
    "        B, T, C, H, W = x_img.shape\n",
    "        x_img = x_img.view(B * T, C, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat_img = self.cnn_backbone(x_img)  # (B*T, 512, 1, 1)\n",
    "        feat_img = feat_img.view(B, T, self.feature_dim_img)\n",
    "        feat_img = feat_img.mean(dim=1)  # (B, 512)\n",
    "\n",
    "        # === Keypointy ===\n",
    "        B, T, N, _ = x_kp.shape\n",
    "        x_kp = x_kp.view(B, T, -1)           # (B, T, N*2)\n",
    "        feat_kp = self.keypoint_mlp(x_kp)    # (B, T, 128)\n",
    "        feat_kp = feat_kp.mean(dim=1)        # (B, 128)\n",
    "\n",
    "        # === Fuzja ===\n",
    "        fused = torch.cat([feat_img, feat_kp], dim=1)  # (B, 640)\n",
    "\n",
    "        out = self.classifier(fused)  # (B, num_actions)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import actions.data_loader as dl\n",
    "# import keypoints.keypoints_boundingbox_approach as kp\n",
    "\n",
    "\n",
    "\n",
    "keypoint_model = None\n",
    "# keypoint_model = kp.KeypointCropModel().to(device)\n",
    "# keypoint_model.load_state_dict(torch.load(\"../../models/bb_23loss_keypoint_crop_model.pth\", map_location=device))\n",
    "# keypoint_model.to(device)\n",
    "# keypoint_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints_batch(model, imgs):\n",
    "    \"\"\"\n",
    "    imgs: (B, T, 3, H, W)\n",
    "    Zwraca: (B, T, N, 2) – keypointy\n",
    "    \"\"\"\n",
    "    B, T, C, H, W = imgs.shape\n",
    "    imgs = imgs.to(device)\n",
    "\n",
    "    keypoints_list = []\n",
    "\n",
    "    for b in range(B):\n",
    "        sample_keypoints = []\n",
    "        for t in range(T):\n",
    "            frame = imgs[b, t].unsqueeze(0)  # (1, 3, H, W)\n",
    "            kp = model(frame)  # np. (1, N, 2)\n",
    "            sample_keypoints.append(kp.squeeze(0).cpu())  # (N, 2)\n",
    "        sample_keypoints = torch.stack(sample_keypoints, dim=0)  # (T, N, 2)\n",
    "        keypoints_list.append(sample_keypoints)\n",
    "\n",
    "    keypoints_tensor = torch.stack(keypoints_list, dim=0)  # (B, T, N, 2)\n",
    "    return keypoints_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusionWrapperDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, base_dataset, keypoint_model):\n",
    "        self.base = base_dataset\n",
    "        self.keypoint_model = keypoint_model\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgs, label = self.base[idx]  # imgs: (T, 3, H, W)\n",
    "        imgs = imgs.unsqueeze(0)  # (1, T, 3, H, W)\n",
    "\n",
    "        keypoints = extract_keypoints_batch(self.keypoint_model, imgs)  # (1, T, N, 2)\n",
    "        keypoints = keypoints.squeeze(0)  # (T, N, 2)\n",
    "\n",
    "        return imgs.squeeze(0), keypoints, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ActionsFusionModel.__init__() got an unexpected keyword argument 'num_classes'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m test_loader_fused  = DataLoader(fusion_test, batch_size=\u001b[32m8\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model = \u001b[43mActionsFusionModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_keypoints\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m17\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m51\u001b[39;49m\u001b[43m)\u001b[49m.to(device)\n",
      "\u001b[31mTypeError\u001b[39m: ActionsFusionModel.__init__() got an unexpected keyword argument 'num_classes'"
     ]
    }
   ],
   "source": [
    "fusion_train = FusionWrapperDataset(dl.dataset_train, keypoint_model)\n",
    "fusion_val   = FusionWrapperDataset(dl.dataset_valid, keypoint_model)\n",
    "fusion_test  = FusionWrapperDataset(dl.dataset_test, keypoint_model)\n",
    "\n",
    "train_loader_fused = DataLoader(fusion_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader_fused   = DataLoader(fusion_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader_fused  = DataLoader(fusion_test, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = ActionsFusionModel(num_keypoints=17, num_classes=51).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss();\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_config = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"train_size\": len(fusion_train),\n",
    "    \"val_size\": len(fusion_val),\n",
    "    \"test_size\": len(fusion_test),\n",
    "    \"model\": \"ActionsBaselineModel\",\n",
    "    \"criterion\": \"Cross entropy loss\",\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"crop_size\": CROP_SIZE,\n",
    "    \"device\": device,\n",
    "    \"data_augmentation\": DATA_AUGMENTATION\n",
    "}\n",
    "\n",
    "wandb.init(\n",
    "    entity=\"fejowo5522-\",\n",
    "    project=\"NN_Project\",\n",
    "    config=wandb_config,\n",
    "    group=\"ActionsBaseline\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_tqdm = True\n",
    "early_stopping = True\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS)):\n",
    "    # print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss = train_one_epoch(model, dl.train_loader, optimizer, criterion, verbose_tqdm=verbose_tqdm)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = evaluate(model, dl.val_loader, criterion, verbose_tqdm=verbose_tqdm)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        'epoch': epoch + 1,\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': val_loss\n",
    "    })\n",
    "    \n",
    "    # Early stopping\n",
    "    if early_stopping:\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
