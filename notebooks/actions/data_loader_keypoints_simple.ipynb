{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "959e0bd2",
   "metadata": {},
   "source": [
    "# NEW VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cadafd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.io import read_image\n",
    "from torchvision import transforms\n",
    "from glob import glob\n",
    "from collections import defaultdict\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f9f9075f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "NUM_FRAMES = 16\n",
    "ROOT_DIR = \"../../data/hmdb51/videos\"\n",
    "SPLIT_DIR = \"../../data/hmdb51/splits\"\n",
    "DATA_AUGMENTATION = True\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "if torch.mps.is_available():\n",
    "    device = \"mps\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "80562520",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "\n",
    "# Wczytanie modelu YOLOv8-pose tylko raz\n",
    "pose_model = YOLO(\"yolov8n-pose.pt\")  # możesz użyć też yolov8s-pose.pt jeśli chcesz większy model\n",
    "pose_model.to(device)\n",
    "\n",
    "def get_yolo_keypoints_from_path(image_path):\n",
    "    \"\"\"\n",
    "    Detekcja keypointów z obrazu (ścieżka).\n",
    "    Zwraca: ndarray shape (17, 2) lub None jeśli brak ludzi.\n",
    "    \"\"\"\n",
    "    results = pose_model.predict(image_path, verbose=False)\n",
    "\n",
    "    if len(results) == 0 or len(results[0].keypoints.xy) == 0:\n",
    "        return None\n",
    "\n",
    "    # Wybierz pierwszą osobę\n",
    "    keypoints = results[0].keypoints.xy[0]  # shape: [17, 2]\n",
    "    return keypoints.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a06c033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hmdb51Dataset(Dataset):\n",
    "    def __init__(self, root_dir, split_dir, split_id=1, split_type=\"train\",\n",
    "                 transform=None, num_frames=NUM_FRAMES,\n",
    "                 selected_classes=None):  # <--- Nowy parametr\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.num_frames = num_frames\n",
    "        self.selected_classes = selected_classes  # <--- zapisz wybrane klasy\n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "        self._load_split(split_dir, split_id, split_type)\n",
    "\n",
    "    def _load_split(self, split_dir, split_id, split_type):\n",
    "        if split_id not in [1, 2, 3]:\n",
    "            print(f\"Warning: Invalid split_id={split_id}. It should be 1, 2, or 3. No data will be loaded.\")\n",
    "            return\n",
    "        if split_type not in [\"train\", \"test\"]:\n",
    "            print(f\"Warning: Invalid split_type='{split_type}'. It should be 'train' or 'test'. No data will be loaded.\")\n",
    "            return\n",
    "\n",
    "        split_files = glob(os.path.join(split_dir, f\"*_test_split{split_id}.txt\"))\n",
    "\n",
    "        # Filtrowanie klas\n",
    "        if not self.class_to_idx:\n",
    "            all_class_names = sorted([os.path.basename(f).split('_test_split')[0] for f in split_files])\n",
    "            if self.selected_classes is not None:\n",
    "                all_class_names = [c for c in all_class_names if c in self.selected_classes]\n",
    "            self.class_to_idx = {name: i for i, name in enumerate(all_class_names)}\n",
    "            self.idx_to_class = {i: name for name, i in self.class_to_idx.items()}\n",
    "\n",
    "        for split_file in sorted(split_files):\n",
    "            class_name = os.path.basename(split_file).split(\"_test_split\")[0]\n",
    "\n",
    "            if self.selected_classes is not None and class_name not in self.selected_classes:\n",
    "                continue  # Pomijamy niechciane klasy\n",
    "\n",
    "            label = self.class_to_idx[class_name]\n",
    "\n",
    "            with open(split_file, \"r\") as f:\n",
    "                for line in f:\n",
    "                    if not line.strip(): continue\n",
    "                    video_name, split_value = line.strip().split()\n",
    "                    split_value = int(split_value)\n",
    "                    target_split = 1 if split_type == \"train\" else 2\n",
    "                    if split_value == target_split:\n",
    "                        video_folder = os.path.join(self.root_dir, class_name, video_name.replace(\".avi\", \"\"))\n",
    "                        if os.path.isdir(video_folder):\n",
    "                            self.samples.append((video_folder, label))\n",
    "\n",
    "        print(f\"Loaded {len(self.samples)} samples for split '{split_type}' (split_id={split_id}). Total classes: {len(self.class_to_idx)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self): raise IndexError(\"List index out of range.\")\n",
    "        folder_path, label = self.samples[idx]\n",
    "\n",
    "        frame_paths = sorted(\n",
    "            f for f in os.listdir(folder_path) \n",
    "            if f.lower().endswith(('.jpg', '.jpeg'))\n",
    "        )\n",
    "        frame_paths = [os.path.join(folder_path, f) for f in frame_paths]\n",
    "\n",
    "        if len(frame_paths) == 0:\n",
    "            raise ValueError(f\"No frames found in: {folder_path}\")\n",
    "\n",
    "        if len(frame_paths) >= self.num_frames:\n",
    "            indices = torch.linspace(0, len(frame_paths) - 1, self.num_frames).long()\n",
    "        else:\n",
    "            indices = torch.arange(len(frame_paths))\n",
    "            indices = torch.cat([\n",
    "                indices,\n",
    "                torch.tensor([len(frame_paths) - 1] * (self.num_frames - len(frame_paths)))\n",
    "            ])\n",
    "\n",
    "        frames_list = []\n",
    "        keypoints_list = []\n",
    "\n",
    "        for i in indices:\n",
    "            path = frame_paths[i]\n",
    "\n",
    "            # Wczytanie obrazu\n",
    "            img = read_image(path)\n",
    "            frames_list.append(img)\n",
    "\n",
    "            # YOLO keypoints (z obrazu na dysku)\n",
    "            kp = get_yolo_keypoints_from_path(path)\n",
    "            # print(kp)\n",
    "            if kp is None or len(kp) != 17:\n",
    "                # print(f\"[WARN] No keypoints detected or wrong shape for: {path}\")\n",
    "                kp = np.zeros((17, 2), dtype=np.float32)\n",
    "            keypoints_list.append(torch.tensor(kp, dtype=torch.float32))\n",
    "\n",
    "        frames = torch.stack(frames_list)  # [T, C, H, W]\n",
    "        keypoints = torch.stack(keypoints_list)  # [T, 17, 2]\n",
    "\n",
    "        if self.transform:\n",
    "            frames = self.transform(frames)\n",
    "\n",
    "        num_classes = len(self.class_to_idx)\n",
    "        one_hot_label = torch.zeros(num_classes, dtype=torch.float32)\n",
    "        one_hot_label[label] = 1.0\n",
    "\n",
    "        return frames, keypoints, one_hot_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fd0a2478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow_grid(img_tensor, title, class_name, label_int):\n",
    "    img_tensor = img_tensor.cpu().clone()\n",
    "    grid_img = img_tensor.permute(1, 2, 0).numpy()\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    grid_img = std * grid_img + mean\n",
    "    grid_img = np.clip(grid_img, 0, 1)\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    plt.imshow(grid_img)\n",
    "    plt.title(f\"{title}\\nKlasa: '{class_name}' (etykieta: {label_int})\", fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "6c5ecd76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 349 samples for split 'train' (split_id=1). Total classes: 5\n",
      "../../data/hmdb51/videos\n",
      "Loaded 150 samples for split 'test' (split_id=1). Total classes: 5\n",
      "150\n",
      "\n",
      "--- Podsumowanie Datasetów ---\n",
      "Zbiór treningowy:     349 próbek\n",
      "Zbiór walidacyjny:   75 próbek\n",
      "Zbiór testowy:          75 próbek\n"
     ]
    }
   ],
   "source": [
    "selected_actions = [\"pullup\", \"pushup\", \"jump\", \"sit\", \"wave\"]\n",
    "# selected_actions = [\"pullup\", \"pushup\", \"jump\"]\n",
    "\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Resize((224, 224), antialias=True),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "if not os.path.isdir(ROOT_DIR):\n",
    "    print(f\"FATAL ERROR: Root directory not found at '{os.path.abspath(ROOT_DIR)}'.\")\n",
    "else:\n",
    "    full_train_dataset = Hmdb51Dataset(\n",
    "        root_dir=ROOT_DIR,\n",
    "        split_dir=SPLIT_DIR,\n",
    "        split_id=1,\n",
    "        split_type=\"train\",\n",
    "        transform=transform,\n",
    "        num_frames=NUM_FRAMES,\n",
    "        selected_classes=selected_actions\n",
    "    )\n",
    "\n",
    "    print(ROOT_DIR)\n",
    "\n",
    "    full_test_dataset = Hmdb51Dataset(\n",
    "        root_dir=ROOT_DIR,\n",
    "        split_dir=SPLIT_DIR,\n",
    "        split_id=1,\n",
    "        split_type=\"test\",\n",
    "        transform=transform,\n",
    "        num_frames=NUM_FRAMES,\n",
    "        selected_classes=selected_actions\n",
    "    )\n",
    "\n",
    "    print(len(full_test_dataset))\n",
    "\n",
    "    generator = torch.Generator().manual_seed(42) \n",
    "\n",
    "    # Parametry\n",
    "    TARGET_RATIO = 1  # Chcemy 10% danych\n",
    "\n",
    "    # 1. Zmniejsz TRAIN do 10%\n",
    "    train_size = int(TARGET_RATIO * len(full_train_dataset))\n",
    "    dataset_train, _ = random_split(\n",
    "        full_train_dataset,\n",
    "        [train_size, len(full_train_dataset) - train_size],\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    VALID_RATIO = 0.5\n",
    "    valid_size = int(VALID_RATIO * len(full_test_dataset))\n",
    "    dataset_valid, rest_to_test = random_split(\n",
    "        full_test_dataset,\n",
    "        [valid_size, len(full_test_dataset) - valid_size],\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    test_size = int(TARGET_RATIO * len(rest_to_test))\n",
    "    dataset_test, _ = random_split(\n",
    "        rest_to_test,\n",
    "        [test_size, len(rest_to_test) - test_size],\n",
    "        generator=generator\n",
    "    )\n",
    "\n",
    "    # test_size = int(0.6 * len(full_test_dataset))\n",
    "    # valid_size = len(full_test_dataset) - test_size\n",
    "    \n",
    "    # generator = torch.Generator().manual_seed(42) \n",
    "    \n",
    "    # dataset_train, _ = random_split(full_train_dataset, [len(full_train_dataset), 0], generator=generator)\n",
    "\n",
    "\n",
    "    # dataset_test, dataset_valid = random_split(full_test_dataset, [test_size, valid_size], generator=generator)\n",
    "\n",
    "    print(\"\\n--- Podsumowanie Datasetów ---\")\n",
    "    print(f\"Zbiór treningowy:     {len(dataset_train)} próbek\")\n",
    "    print(f\"Zbiór walidacyjny:   {len(dataset_valid)} próbek\")\n",
    "    print(f\"Zbiór testowy:          {len(dataset_test)} próbek\")\n",
    "\n",
    "    train_loader = DataLoader(dataset_train, batch_size=4, shuffle=True)\n",
    "    val_loader = DataLoader(dataset_valid, batch_size=4, shuffle=False)\n",
    "    test_loader = DataLoader(dataset_test, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f762e648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it = 0\n",
    "# for x in train_loader:\n",
    "#     it += 1\n",
    "#     print(it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6ee40815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if len(dataset_train) > 0:\n",
    "#     print(f\"\\n--- Weryfikacja nowej próbki ---\")\n",
    "    \n",
    "#     frames, one_hot_label = dataset_train[7]\n",
    "    \n",
    "#     print(f\"Kształt tensora klatek (X): {frames.shape}\")\n",
    "#     print(f\"Kształt etykiety one-hot (Y): {one_hot_label.shape}\")\n",
    "#     print(f\"Fragment etykiety one-hot: {one_hot_label[:10]}...\")\n",
    "    \n",
    "#     label_int = torch.argmax(one_hot_label).item()\n",
    "#     class_name = dataset_train.dataset.idx_to_class[label_int]\n",
    "    \n",
    "#     print(f\"Odzyskana etykieta: {label_int}, Klasa: {class_name}\")\n",
    "\n",
    "#     grid = torchvision.utils.make_grid(frames, nrow=8, padding=2)\n",
    "#     imshow_grid(grid, title=\"Przykładowe 16 klatek z jednego wideo\", class_name=class_name, label_int=label_int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1c281d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# if len(dataset_test) > 0:\n",
    "#     print(f\"\\n--- Weryfikacja nowej próbki ---\")\n",
    "    \n",
    "#     frames, one_hot_label = dataset_test[7]\n",
    "    \n",
    "#     print(f\"Kształt tensora klatek (X): {frames.shape}\")\n",
    "#     print(f\"Kształt etykiety one-hot (Y): {one_hot_label.shape}\")\n",
    "#     print(f\"Fragment etykiety one-hot: {one_hot_label[:10]}...\")\n",
    "\n",
    "#     # Zakładamy, że dataset_test.dataset.class_to_idx istnieje\n",
    "#     # class_to_idx = dataset_test.dataset.class_to_idx\n",
    "#     # idx_to_class = {v: k for k, v in class_to_idx.items()}\n",
    "\n",
    "#     # label_int = torch.argmax(one_hot_label).item()\n",
    "#     # class_name = idx_to_class[label_int]\n",
    "#     # # label_int = torch.argmax(one_hot_label).item()\n",
    "#     # # class_name = dataset_test.dataset.idx_to_class[label_int]\n",
    "    \n",
    "#     # print(f\"Odzyskana etykieta: {label_int}, Klasa: {class_name}\")\n",
    "\n",
    "#     # grid = torchvision.utils.make_grid(frames, nrow=8, padding=2)\n",
    "#     # imshow_grid(grid, title=\"Przykładowe 16 klatek z jednego wideo\", class_name=class_name, label_int=label_int)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
